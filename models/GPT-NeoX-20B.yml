framework:
  name: 'Model Openness Framework'
  version: '1.0'
  date: '2024-12-15'
release:
  name: GPT-NeoX-20B
  version: 20B
  date: '2025-05-30'
  type: language
  architecture: 'transformer decoder'
  origin: 'GPT Megatron'
  producer: EleutherAI
  contact: ''
  repository: 'https://github.com/EleutherAI/gpt-neox'
  huggingface: 'https://huggingface.co/EleutherAI/gpt-neox-20b'
  license:
    distribution:
      name: Apache-2.0
      path: ''
  components:
    -
      name: 'Model architecture'
      description: "Well commented code for the model's architecture"
    -
      name: 'Data preprocessing code'
      description: 'Code for data cleansing, normalization, and augmentation'
    -
      name: 'Training code'
      description: 'Code used for training the model'
    -
      name: 'Inference code'
      description: 'Code used for running the model to make predictions'
    -
      name: 'Evaluation code'
      description: 'Code used for evaluating the model'
    -
      name: 'Model parameters (Final)'
      description: 'Trained model parameters, weights and biases'
    -
      name: 'Model parameters (Intermediate)'
      description: 'Trained model parameters, weights and biases'
    -
      name: Datasets
      description: 'Training, validation and testing datasets used for the model'
    -
      name: 'Model metadata'
      description: 'Any model metadata including training configuration and optimizer states'
    -
      name: 'Model card'
      description: 'Model details including performance metrics, intended use, and limitations'
    -
      name: 'Data card'
      description: 'Documentation for datasets including source, characteristics, and preprocessing details'
    -
      name: 'Research paper'
      description: 'Research paper detailing the development and capabilities of the model'
      component_path: 'https://arxiv.org/abs/2204.06745'
      license: CC-BY-4.0
    -
      name: 'Evaluation results'
      description: 'The results from evaluating the model'
